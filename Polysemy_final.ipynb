{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMYhxXMegxuNxU+2Q69H4Vr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imrishabhyadav/NLP-Polysemy/blob/main/Polysemy_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0gicF7Ht4Ax",
        "outputId": "927f55db-4da3-4ec8-db79-8fe5c712b467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Collecting wordnet\n",
            "  Using cached wordnet-0.0.1b2.tar.gz (8.8 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "sentence_1_polysemy:\n",
            "  words_and_senses:\n",
            "    - ('Turn', 'a circular segment of a curve')\n",
            "    - ('light', '(physics) electromagnetic radiation that can produce a visual sensation')\n",
            "  contextual_predictions:\n",
            "    Turn:\n",
            "      - {'score': 0.3366808593273163, 'token': 2735, 'token_str': 'turn', 'sequence': 'turn the light on'}\n",
            "      - {'score': 0.10120954364538193, 'token': 12342, 'token_str': 'shine', 'sequence': 'shine the light on'}\n",
            "      - {'score': 0.04098863527178764, 'token': 2357, 'token_str': 'turned', 'sequence': 'turned the light on'}\n",
            "      - {'score': 0.03962360695004463, 'token': 2681, 'token_str': 'leave', 'sequence': 'leave the light on'}\n",
            "      - {'score': 0.03153924643993378, 'token': 2718, 'token_str': 'hit', 'sequence': 'hit the light on'}\n",
            "    light:\n",
            "      - {'score': 0.12967431545257568, 'token': 2422, 'token_str': 'light', 'sequence': 'turn the light on'}\n",
            "      - {'score': 0.06076744943857193, 'token': 17446, 'token_str': 'ignition', 'sequence': 'turn the ignition on'}\n",
            "      - {'score': 0.055537473410367966, 'token': 4597, 'token_str': 'lights', 'sequence': 'turn the lights on'}\n",
            "      - {'score': 0.04362677037715912, 'token': 2557, 'token_str': 'radio', 'sequence': 'turn the radio on'}\n",
            "      - {'score': 0.04302795231342316, 'token': 2694, 'token_str': 'tv', 'sequence': 'turn the tv on'}\n",
            "\n",
            "sentence_2_polysemy:\n",
            "  words_and_senses:\n",
            "    - ('box', 'a (usually rectangular) container; may have a lid')\n",
            "    - ('light', '(physics) electromagnetic radiation that can produce a visual sensation')\n",
            "    - ('weight', 'the vertical force exerted by a mass as a result of gravity')\n",
            "  contextual_predictions:\n",
            "    box:\n",
            "      - {'score': 0.045175570994615555, 'token': 2160, 'token_str': 'house', 'sequence': 'the house was very light in weight'}\n",
            "      - {'score': 0.04174026846885681, 'token': 2282, 'token_str': 'room', 'sequence': 'the room was very light in weight'}\n",
            "      - {'score': 0.03905315697193146, 'token': 2911, 'token_str': 'ship', 'sequence': 'the ship was very light in weight'}\n",
            "      - {'score': 0.03558846563100815, 'token': 2311, 'token_str': 'building', 'sequence': 'the building was very light in weight'}\n",
            "      - {'score': 0.030959507450461388, 'token': 2482, 'token_str': 'car', 'sequence': 'the car was very light in weight'}\n",
            "    light:\n",
            "      - {'score': 0.6238505244255066, 'token': 2422, 'token_str': 'light', 'sequence': 'the box was very light in weight'}\n",
            "      - {'score': 0.16795752942562103, 'token': 2235, 'token_str': 'small', 'sequence': 'the box was very small in weight'}\n",
            "      - {'score': 0.04939576983451843, 'token': 3082, 'token_str': 'heavy', 'sequence': 'the box was very heavy in weight'}\n",
            "      - {'score': 0.031300753355026245, 'token': 2659, 'token_str': 'low', 'sequence': 'the box was very low in weight'}\n",
            "      - {'score': 0.02679109014570713, 'token': 2312, 'token_str': 'large', 'sequence': 'the box was very large in weight'}\n",
            "    weight:\n",
            "      - {'score': 0.6875457167625427, 'token': 1012, 'token_str': '.', 'sequence': 'the box was very light in.'}\n",
            "      - {'score': 0.30948349833488464, 'token': 1025, 'token_str': ';', 'sequence': 'the box was very light in ;'}\n",
            "      - {'score': 0.00205432903021574, 'token': 999, 'token_str': '!', 'sequence': 'the box was very light in!'}\n",
            "      - {'score': 0.0008278436725959182, 'token': 1029, 'token_str': '?', 'sequence': 'the box was very light in?'}\n",
            "      - {'score': 3.264818587922491e-05, 'token': 1064, 'token_str': '|', 'sequence': 'the box was very light in |'}\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers nltk\n",
        "!pip install wordnet\n",
        "!pip install nltk\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet as wn\n",
        "from transformers import pipeline\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Set up pipeline\n",
        "fill_mask = pipeline('fill-mask', model='bert-base-uncased', tokenizer='bert-base-uncased')\n",
        "\n",
        "# Load stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "def get_word_sense(word):\n",
        "    synsets = wn.synsets(word)\n",
        "    if synsets:\n",
        "        return synsets[0].definition()\n",
        "    return \"No definition found.\"\n",
        "\n",
        "def contextualize_polysemy(sentence, word):\n",
        "    sentence_with_mask = sentence.replace(word, '[MASK]')\n",
        "    predictions = fill_mask(sentence_with_mask)\n",
        "    return predictions\n",
        "\n",
        "def detect_polysemy_and_describe(sentence1, sentence2):\n",
        "    polysemy_details = {}\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    tokens_sentence1 = remove_stopwords(word_tokenize(sentence1))\n",
        "    tokens_sentence2 = remove_stopwords(word_tokenize(sentence2))\n",
        "\n",
        "    # Get WordNet senses\n",
        "    polysemy_1 = [(word, get_word_sense(word)) for word in tokens_sentence1 if wn.synsets(word)]\n",
        "    polysemy_2 = [(word, get_word_sense(word)) for word in tokens_sentence2 if wn.synsets(word)]\n",
        "\n",
        "    # Contextual predictions using BERT\n",
        "    contextual_sentence1 = {word: contextualize_polysemy(sentence1, word) for word, _ in polysemy_1}\n",
        "    contextual_sentence2 = {word: contextualize_polysemy(sentence2, word) for word, _ in polysemy_2}\n",
        "\n",
        "    # Store results\n",
        "    polysemy_details['sentence_1_polysemy'] = {\n",
        "        'words_and_senses': polysemy_1,\n",
        "        'contextual_predictions': contextual_sentence1\n",
        "    }\n",
        "    polysemy_details['sentence_2_polysemy'] = {\n",
        "        'words_and_senses': polysemy_2,\n",
        "        'contextual_predictions': contextual_sentence2\n",
        "    }\n",
        "\n",
        "    return polysemy_details\n",
        "\n",
        "# Example sentences\n",
        "sentence1 = \"Turn the light on\"\n",
        "sentence2 = \"The box was very light in weight\"\n",
        "\n",
        "result = detect_polysemy_and_describe(sentence1, sentence2)\n",
        "\n",
        "# Pretty print\n",
        "for key, value in result.items():\n",
        "    print(f\"\\n{key}:\")\n",
        "    for subkey, subvalue in value.items():\n",
        "        print(f\"  {subkey}:\")\n",
        "        if isinstance(subvalue, dict):\n",
        "            for word, predictions in subvalue.items():\n",
        "                print(f\"    {word}:\")\n",
        "                for pred in predictions:\n",
        "                    print(f\"      - {pred}\")\n",
        "        else:\n",
        "            for item in subvalue:\n",
        "                print(f\"    - {item}\")\n"
      ]
    }
  ]
}